#!/usr/bin/env bash
set -euo pipefail

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SERVER="france"
REMOTE_WEB="/var/www/isthelclcpoolopen.ca/html"
# On the remote side, use the SSH userâ€™s $HOME
REMOTE_SCRAPERS="\$HOME/scrapers"

BUILD_DIR="build"
APP_TAR="app.tar.gz"
SCRAPERS_TAR="scrapers.tar.gz"

# â”€â”€â”€ CLEANUP ON EXIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cleanup(){
  rm -f "$APP_TAR" "$SCRAPERS_TAR"
}
trap cleanup EXIT

# â”€â”€â”€ 1) BUILD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ”§ Running React buildâ€¦"
npm run build
[ -d "$BUILD_DIR" ] || { echo "âŒ Build failed: '$BUILD_DIR' not found"; exit 1; }

# â”€â”€â”€ 2) PACKAGE TARBALLS (no xattrs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ“¦ Packaging web app â†’ $APP_TAR"
COPYFILE_DISABLE=1 tar -C "$BUILD_DIR" -czf "$APP_TAR" .

echo "ğŸ“¦ Packaging scrapers â†’ $SCRAPERS_TAR"
COPYFILE_DISABLE=1 tar \
  --exclude='node_modules' \
  --exclude='data/*.json' \
  -C scrapers -czf "$SCRAPERS_TAR" .

# â”€â”€â”€ 3) UPLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸš€ Uploading artifacts to $SERVERâ€¦"
scp "$APP_TAR" "$SCRAPERS_TAR" "$SERVER":/tmp/

# â”€â”€â”€ 4) REMOTE DEPLOY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ssh "$SERVER" bash <<'EOF'
set -euo pipefail

# Remoteâ€side variables:
REMOTE_WEB="/var/www/isthelclcpoolopen.ca/html"
REMOTE_DATA="/var/www/isthelclcpoolopen.ca/data"
REMOTE_SCRAPERS="\$HOME/scrapers"
APP_TAR="app.tar.gz"
SCRAPERS_TAR="scrapers.tar.gz"

echo "ğŸ‘‰ Clearing & extracting web appâ€¦"
rm -rf "\$REMOTE_WEB"/*
mkdir -p "\$REMOTE_WEB"
tar --warning=no-unknown-keyword -xzf /tmp/\$APP_TAR -C "\$REMOTE_WEB" 2>/dev/null
rm /tmp/\$APP_TAR
chmod -R 775 "\$REMOTE_WEB"

echo "ğŸ‘‰ Deploying scrapersâ€¦"
rm -rf "\$REMOTE_SCRAPERS"
mkdir -p "\$REMOTE_SCRAPERS/logs" "\$REMOTE_SCRAPERS/data"
tar --warning=no-unknown-keyword -xzf /tmp/\$SCRAPERS_TAR -C "\$REMOTE_SCRAPERS" 2>/dev/null
rm /tmp/\$SCRAPERS_TAR

cd "\$REMOTE_SCRAPERS"
npm install --omit=dev --no-audit --no-fund --loglevel=error

echo "ğŸ‘‰ Ensuring data directory existsâ€¦"
mkdir -p "\$REMOTE_DATA"
chmod 775 "\$REMOTE_DATA"

echo "ğŸ‘‰ Ensuring crontab entriesâ€¦"
declare -a CRONS=(
  # pool scraper every 30m
  "*/30 * * * *  cd \$REMOTE_SCRAPERS && DATA_PATH=\$REMOTE_DATA node pool-scraper.js >> \$REMOTE_SCRAPERS/logs/pool-scraper.log 2>&1"
  # skating scraper daily at 01:00
  "0 1 * * *     cd \$REMOTE_SCRAPERS && DATA_PATH=\$REMOTE_DATA node skating-scraper.js >> \$REMOTE_SCRAPERS/logs/skating-scraper.log 2>&1"
  # libraries scraper daily at 02:00
  "0 2 * * *     cd \$REMOTE_SCRAPERS && DATA_PATH=\$REMOTE_DATA node libraries-scraper.js >> \$REMOTE_SCRAPERS/logs/libraries-scraper.log 2>&1"
)

( crontab -l 2>/dev/null \
    | grep -Fv -f <(printf "%s\n" "\${CRONS[@]}") \
  ; printf "%s\n" "\${CRONS[@]}" ) \
  | crontab -

echo "âœ… Deployment complete!"
EOF

echo "ğŸ‰ All done!"
